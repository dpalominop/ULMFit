{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.xmlreader as xml\n",
    "import lib.utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import gensim.models.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = xml.readXML(\"../database/TASS/TASS2017/task1-Training.xml\",[0,1,2,3])\n",
    "val_docs   = xml.readXML(\"../database/TASS/TASS2017/task1-Development.xml\",[0,1,2,3])\n",
    "test_docs  = xml.readXML(\"../database/TASS/TASS2017/task1-Test.xml\",[0,1,2,3])\n",
    "general_docs = xml.readXML2(\"../database/TASS/TASS2017/task1-General.xml\",[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []\n",
    "train_labels = []\n",
    "for doc in train_docs:\n",
    "    # train_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    train_tweets.append(doc.content)\n",
    "    train_labels.append(doc.polarity)\n",
    "\n",
    "val_tweets = []\n",
    "val_labels = []\n",
    "for doc in val_docs:\n",
    "    # test_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    val_tweets.append(doc.content)\n",
    "    val_labels.append(doc.polarity)\n",
    "    \n",
    "test_tweets = []\n",
    "test_labels = []\n",
    "for doc in test_docs:\n",
    "    # test_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    test_tweets.append(doc.content)\n",
    "    test_labels.append(doc.polarity)\n",
    "    \n",
    "general_tweets = []\n",
    "general_labels = []\n",
    "for doc in general_docs:\n",
    "    # test_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    general_tweets.append(doc.content)\n",
    "    general_labels.append(doc.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008, 506, 1899, 7218)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tweets), len(val_tweets), len(test_tweets), len(general_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattern(A):\n",
    "    'Source: https://stackoverflow.com/a/17867797/7273299'\n",
    "    \n",
    "    'Flattens a list of lists and strings into a list.'\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flattern(i))\n",
    "        else: rt.append(i)\n",
    "    return rt\n",
    "\n",
    "def isInt(v):\n",
    "    'Source: https://stackoverflow.com/a/9859202/7273299'\n",
    "    \n",
    "    'Checks if a string is a number.'\n",
    "    try:     i = int(v)\n",
    "    except:  return False\n",
    "    return True\n",
    "\n",
    "def char_count(word, chars, lbound=2):\n",
    "    char_count = [word.count(char) for char in chars]\n",
    "    return all(i >= lbound for i in char_count)\n",
    "\n",
    "def replace_lol(repl_str, texts):\n",
    "    for string, chars in repl_str:\n",
    "        texts = [[[string, i] if char_count(i, set(chars), 2) else i for i in text.split()] for text in texts]\n",
    "        texts = np.array([flattern(text) for text in texts])\n",
    "        texts = np.array([' '.join(text) for text in texts])\n",
    "    return texts\n",
    "\n",
    "def preprocess_tweets(tweets):\n",
    "    \"\"\"\n",
    "    twitter specific text processing and shuffle\n",
    "    \"\"\"\n",
    "    # Placeholders for hyperlinks and user references\n",
    "    tweets = [['hyp_link' if i.startswith('http') \n",
    "               else 'user_ref' if i.startswith('@')\n",
    "               else i for i in tweet.split()] for tweet in tweets]\n",
    "    tweets = np.array([' '.join(i) for i in tweets])\n",
    "\n",
    "    # Prefix for Hashtags\n",
    "    tweets = [[['hash_tag', i] if i.startswith('#') else i for i in tweet.split()] for tweet in tweets]\n",
    "    tweets = np.array([flattern(tweet) for tweet in tweets])\n",
    "    tweets = np.array([' '.join(i) for i in tweets])\n",
    "\n",
    "    # Prefix for integers\n",
    "    tweets = [[['int_string', i] if isInt(i) else i for i in tweet.split()] for tweet in tweets]\n",
    "    tweets = np.array([flattern(tweet) for tweet in tweets])\n",
    "    tweets = np.array([' '.join(i) for i in tweets])\n",
    "\n",
    "    # Prefix for slang\n",
    "    tweets = [[['que', 'slang_string'] if i=='q' else ['por', 'slang_string'] if i=='x' else ['de', 'slang_string'] if i=='d' else ['Que', 'slang_string'] if i=='Q' else ['Por', 'slang_string'] if i=='X' else ['De', 'slang_string'] if i=='D' else i for i in tweet.split()] for tweet in tweets]\n",
    "    tweets = np.array([flattern(tweet) for tweet in tweets])\n",
    "    tweets = np.array([' '.join(i) for i in tweets])\n",
    "\n",
    "    # Lol type characters\n",
    "    repl_str = [('risa_ja','ja'), ('risa_ji','ji'), ('risa_je','je'), ('risa_jo','jo'), ('risa_ju', 'ju'),\n",
    "               ('risa_ja','aj'), ('risa_ji','ij'), ('risa_ju', 'uj')]\n",
    "\n",
    "    # Adding prefix to lol type characters\n",
    "    tweets = replace_lol(repl_str, tweets)\n",
    "\n",
    "   \n",
    "    \n",
    "    return tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = preprocess_tweets(train_tweets)\n",
    "val_tweets   = preprocess_tweets(val_tweets)\n",
    "test_tweets  = preprocess_tweets(test_tweets)\n",
    "general_tweets  = preprocess_tweets(general_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['label', 'tweet']\n",
    "df_trn = pd.DataFrame({'tweet':train_tweets, 'label':train_labels},\n",
    "                      columns=col_names)\n",
    "\n",
    "df_val = pd.DataFrame({'tweet':val_tweets, 'label':val_labels},\n",
    "                      columns=col_names)\n",
    "\n",
    "df_test = pd.DataFrame({'tweet':test_tweets, 'label':test_labels},\n",
    "                      columns=col_names)\n",
    "df_general = pd.DataFrame({'tweet':general_tweets, 'label':general_labels},\n",
    "                      columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Salgo de #VeoTV , que día más largoooooo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @FabHddzC: Si amas a alguien, déjalo libre....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Hoy asisitiré en Madrid a un seminario sobre l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Desde el escaño. Todo listo para empezar #endi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Bdías. EM no se ira de puente. Si vosotros os ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Un sistema económico q recorta dinero para pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>#programascambiados caca d ajuste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>Buen viernes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>“@adri_22_22: #programascambiados es TT gracia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>Noooooos días! Me he dormidoooooo ya estoy en ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>Vamos a por el viernes (@ Ayuntamiento de Mála...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>La Universidad confía en De la Calle para enca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>¿Me ayudáis a que #indultoneiro sea TT? Por si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>abcdesevilla.es: Recio no tiene «indicios pote...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet\n",
       "0       3        Salgo de #VeoTV , que día más largoooooo...\n",
       "1       2  @PauladeLasHeras No te libraras de ayudar me/n...\n",
       "2       1                          @marodriguezb Gracias MAR\n",
       "3       0  Off pensando en el regalito Sinde, la que se v...\n",
       "4       1  Conozco a alguien q es adicto al drama! Ja ja ...\n",
       "5       3  RT @FabHddzC: Si amas a alguien, déjalo libre....\n",
       "6       1  Toca @crackoviadeTV3 . Grabación dl especial N...\n",
       "7       3  Hoy asisitiré en Madrid a un seminario sobre l...\n",
       "8       1  Buen día todos! Lo primero mandar un abrazo gr...\n",
       "9       1  Desde el escaño. Todo listo para empezar #endi...\n",
       "10      1  Bdías. EM no se ira de puente. Si vosotros os ...\n",
       "11      1  Un sistema económico q recorta dinero para pre...\n",
       "12      0                  #programascambiados caca d ajuste\n",
       "13      1                                       Buen viernes\n",
       "14      1  “@adri_22_22: #programascambiados es TT gracia...\n",
       "15      3  Noooooos días! Me he dormidoooooo ya estoy en ...\n",
       "16      3  Vamos a por el viernes (@ Ayuntamiento de Mála...\n",
       "17      1  La Universidad confía en De la Calle para enca...\n",
       "18      1  ¿Me ayudáis a que #indultoneiro sea TT? Por si...\n",
       "19      0  abcdesevilla.es: Recio no tiene «indicios pote..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_general.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn.to_csv('../database/TASS/TASS2017/train.csv', header=False, index=False)\n",
    "df_val.to_csv('../database/TASS/TASS2017/validation.csv', header=False, index=False)\n",
    "df_test.to_csv('../database/TASS/TASS2017/test.csv', header=False, index=False)\n",
    "df_general.to_csv('../database/TASS/TASS2017/general.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentences = 242\n",
      "Negative Sentences = 231\n",
      "Neutral  Sentences = 166\n",
      "None Values        = 361\n"
     ]
    }
   ],
   "source": [
    "POSI_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 0]\n",
    "NEGA_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 1]\n",
    "NEUT_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 2]\n",
    "NONE_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 3]\n",
    "\n",
    "level_train_docs = [POSI_train_docs,NEGA_train_docs,NEUT_train_docs,NONE_train_docs]\n",
    "\n",
    "fmt = \"\"\"Positive Sentences = {:d}\n",
    "       \\rNegative Sentences = {:d}\n",
    "       \\rNeutral  Sentences = {:d}\n",
    "       \\rNone Values        = {:d}\"\"\"\n",
    "\n",
    "print(fmt.format(len(POSI_train_docs),\n",
    "                 len(NEGA_train_docs),\n",
    "                 len(NEUT_train_docs),\n",
    "                 len(NONE_train_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of sentences per level :  166\n"
     ]
    }
   ],
   "source": [
    "minSentLvl = min(len(POSI_train_docs),len(NEGA_train_docs),len(NEUT_train_docs),len(NONE_train_docs))\n",
    "\n",
    "print('Minimum number of sentences per level : ', minSentLvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "new_train_docs = []\n",
    "for i in range(len(level_train_docs)):\n",
    "    level_per = random.sample(level_train_docs[i],len(level_train_docs[i]))\n",
    "    new_train_docs.append(level_per[:minSentLvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New size of sentences:\n",
      "\n",
      "Positive Sentences = 166\n",
      "Negative Sentences = 166\n",
      "Neutral  Sentences = 166\n",
      "None Values        = 166\n"
     ]
    }
   ],
   "source": [
    "print(\"New size of sentences:\\n\")\n",
    "fmt = \"\"\"Positive Sentences = {:d}\n",
    "       \\rNegative Sentences = {:d}\n",
    "       \\rNeutral  Sentences = {:d}\n",
    "       \\rNone Values        = {:d}\"\"\"\n",
    "\n",
    "print(fmt.format(len(new_train_docs[0]),\n",
    "                 len(new_train_docs[1]),\n",
    "                 len(new_train_docs[2]),\n",
    "                 len(new_train_docs[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuf_train_docs size =  664\n"
     ]
    }
   ],
   "source": [
    "flat_train_docs = [item for sublist in new_train_docs for item in sublist]\n",
    "shuf_train_docs = random.sample(flat_train_docs,len(flat_train_docs))\n",
    "\n",
    "assert (len(shuf_train_docs) == 4 * minSentLvl)\n",
    "print(\"shuf_train_docs size = \", len(shuf_train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for doc in shuf_train_docs + val_docs:\n",
    "    corpus.append(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences =  1164\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences = \", (len(val_docs + shuf_train_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuf_train_labels = []\n",
    "for doc in shuf_train_docs:\n",
    "    shuf_train_labels.append(doc.polarity)\n",
    "    \n",
    "assert (len(shuf_train_labels) == len(shuf_train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer(tokenizer=ut.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164, 4653)\n"
     ]
    }
   ],
   "source": [
    "X = counter.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs   = xml.readXMLTest(\"../database/TASS/TASS2018/task1-Test.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = []\n",
    "for doc in test_docs:\n",
    "    test_tweets.append(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428\n"
     ]
    }
   ],
   "source": [
    "print(len(test_tweets))\n",
    "# assert (len(test_tweets) == 1899)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for tweet in corpus + test_tweets:\n",
    "    sentence = []\n",
    "    for word in ut.tokenizer(tweet):\n",
    "        sentence.append(word)\n",
    "    sequences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(sequences) == (len(shuf_train_docs) + len(val_docs) + len(test_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter(word for doc in sequences for word in doc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 8103)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt['felicidad'], len(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('activacion', 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.most_common()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('que', 1458),\n",
       " ('de', 1233),\n",
       " ('y', 1045),\n",
       " ('a', 901),\n",
       " ('no', 878),\n",
       " ('la', 820),\n",
       " ('me', 739),\n",
       " ('el', 725),\n",
       " ('en', 647),\n",
       " ('es', 567)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8105"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "max_vocab = len(cnt)\n",
    "min_freq = 0\n",
    "#max_vocab = len(cnt)\n",
    "#max_vocab = 4000\n",
    "#min_freq  = 0\n",
    "\n",
    "\n",
    "itos = [o for o,c in cnt.most_common(max_vocab) if c >= min_freq]\n",
    "\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8103\n"
     ]
    }
   ],
   "source": [
    "print(len(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8105"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "TASS_     = '2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(itos, open('../database/ulmfit/tmp/itos_'+TASS_+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos      = pickle.load(open('../database/ulmfit/tmp/itos_'+TASS_+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = [[stoi[o] for o in p] for p in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(sequences) == len(data_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data train tensor: (664,)\n",
      "Shape of data val  tensor: (500,)\n",
      "Shape of data test  tensor: (1428,)\n"
     ]
    }
   ],
   "source": [
    "x_train_ids = np.array(data_ids[:len(shuf_train_labels)])\n",
    "x_val_ids   = np.array(data_ids[len(shuf_train_labels):len(shuf_train_labels)+len(val_docs)])\n",
    "x_test_ids  = np.array(data_ids[(len(shuf_train_labels)+len(val_docs)):])\n",
    "\n",
    "print('Shape of data train tensor:', x_train_ids.shape)\n",
    "print('Shape of data val  tensor:', x_val_ids.shape)\n",
    "print('Shape of data test  tensor:', x_test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = '../database/ulmfit/'\n",
    "\n",
    "np.save(SAVE_PATH + \"tmp/train_labels_\"+TASS_+\".npy\",shuf_train_labels)\n",
    "np.save(SAVE_PATH + \"tmp/val_labels_\"+TASS_+\".npy\",val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(SAVE_PATH + \"tmp/train_ids_\"+TASS_+\".npy\",x_train_ids)\n",
    "np.save(SAVE_PATH + \"tmp/val_ids_\"+TASS_+\".npy\",x_val_ids)\n",
    "np.save(SAVE_PATH + \"tmp/test_ids_\"+TASS_+\".npy\",x_test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['necesito',\n",
       " 'que',\n",
       " 'sea',\n",
       " 'viernes',\n",
       " 'para',\n",
       " 'que',\n",
       " 'paguen',\n",
       " 'estoy',\n",
       " 'en',\n",
       " 'la',\n",
       " 'miseria',\n",
       " 'tota']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unk_',\n",
       " '_pad_',\n",
       " 'que',\n",
       " 'de',\n",
       " 'y',\n",
       " 'a',\n",
       " 'no',\n",
       " 'la',\n",
       " 'me',\n",
       " 'el',\n",
       " 'en',\n",
       " 'es',\n",
       " 'mi',\n",
       " 'lo',\n",
       " 'un',\n",
       " 'se',\n",
       " 'por',\n",
       " 'con',\n",
       " 'te',\n",
       " 'los',\n",
       " 'pero',\n",
       " 'ya',\n",
       " 'si',\n",
       " 'una',\n",
       " 'para',\n",
       " 'yo',\n",
       " 'mas',\n",
       " 'las',\n",
       " 'como',\n",
       " 'o',\n",
       " 'al',\n",
       " 'todo',\n",
       " 'tu',\n",
       " 'solo',\n",
       " 'esta',\n",
       " 'cuando',\n",
       " 'del',\n",
       " 'quiero',\n",
       " 'dia',\n",
       " 'porque',\n",
       " 'estoy',\n",
       " 'hoy',\n",
       " 'mejor',\n",
       " 'muy',\n",
       " 'tengo',\n",
       " 'feliz',\n",
       " 'mis',\n",
       " 'bueno',\n",
       " 'eso',\n",
       " 'q',\n",
       " 'su',\n",
       " 'ser',\n",
       " 'siempre',\n",
       " 'bien',\n",
       " 'le',\n",
       " 'extra',\n",
       " 'ahora',\n",
       " 'este',\n",
       " 'sin',\n",
       " 'buen',\n",
       " 'gracias',\n",
       " 'ver',\n",
       " 'ni',\n",
       " 'todos',\n",
       " 'asi',\n",
       " 'hay',\n",
       " 'tan',\n",
       " 'hace',\n",
       " 'vida',\n",
       " 'os',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'fue',\n",
       " 'ao',\n",
       " 'nos',\n",
       " 'eres',\n",
       " 'ma',\n",
       " 'hacer',\n",
       " 'nada',\n",
       " 'hasta',\n",
       " 'ir',\n",
       " 'mucho',\n",
       " 'he',\n",
       " 'dias',\n",
       " 'buena',\n",
       " 'cosas',\n",
       " 'estar',\n",
       " 'desde',\n",
       " 'vez',\n",
       " 'algo',\n",
       " 'sea',\n",
       " 'igual',\n",
       " 'nuevo',\n",
       " 'ana',\n",
       " 'tambien',\n",
       " 'ese',\n",
       " 'espero',\n",
       " 'sus',\n",
       " 'voy',\n",
       " 'tiempo',\n",
       " 'peru',\n",
       " 'tus',\n",
       " 'creo',\n",
       " 'semana',\n",
       " 'sera',\n",
       " 'era',\n",
       " 'aun',\n",
       " 'esa',\n",
       " 'ha',\n",
       " 'menos',\n",
       " 'siento',\n",
       " 'cada',\n",
       " 'mal',\n",
       " 'lindo',\n",
       " 'tiene',\n",
       " 'donde',\n",
       " 'va',\n",
       " 'uno',\n",
       " 'hola',\n",
       " 'ti',\n",
       " 'puedo',\n",
       " 'nadie',\n",
       " 'estas',\n",
       " 'tener',\n",
       " 'sue',\n",
       " 'jajajaja',\n",
       " 'da',\n",
       " 'veo',\n",
       " 'nunca',\n",
       " 'peor',\n",
       " 'casa',\n",
       " 'pues',\n",
       " 'aqui',\n",
       " 'amor',\n",
       " 'les',\n",
       " 'dios',\n",
       " 'estan',\n",
       " 'tienes',\n",
       " 'bonito',\n",
       " 'fin',\n",
       " 'alguien',\n",
       " 'momento',\n",
       " 'serio',\n",
       " 'salir',\n",
       " 'toda',\n",
       " 'cumplea',\n",
       " 'mismo',\n",
       " 'saludos',\n",
       " 'jaja',\n",
       " 'jajaja',\n",
       " 'gran',\n",
       " 'pasa',\n",
       " 'familia',\n",
       " 'final',\n",
       " 'ojala',\n",
       " 'persona',\n",
       " 'mama',\n",
       " 'navidad',\n",
       " 'corazon',\n",
       " 'paso',\n",
       " 'mundo',\n",
       " 'ah',\n",
       " 'triste',\n",
       " 'otro',\n",
       " 'quien',\n",
       " 'dos',\n",
       " 'luego',\n",
       " 'otra',\n",
       " 'antes',\n",
       " 'gente',\n",
       " 'muchas',\n",
       " 'dormir',\n",
       " 'ayer',\n",
       " 'esto',\n",
       " 'ultimo',\n",
       " 'dice',\n",
       " 'decir',\n",
       " 'gusta',\n",
       " 'tanto',\n",
       " 'quiere',\n",
       " 'ella',\n",
       " 'amiga',\n",
       " 'vamos',\n",
       " 'noche',\n",
       " 'cuenta',\n",
       " 'verdad',\n",
       " 'van',\n",
       " 'amigo',\n",
       " 'buenos',\n",
       " 'hora',\n",
       " 'amo',\n",
       " 'puede',\n",
       " 'claro',\n",
       " 'hecho',\n",
       " 'di',\n",
       " 'amigos',\n",
       " 'seguro',\n",
       " 'genial',\n",
       " 'tarde',\n",
       " 'unico',\n",
       " 'personas',\n",
       " 'mejores',\n",
       " 'primera',\n",
       " 'todas',\n",
       " 'trabajo',\n",
       " 'seria',\n",
       " 'papa',\n",
       " 'rico',\n",
       " 'malo',\n",
       " 'despues',\n",
       " 'as',\n",
       " 'ganas',\n",
       " 'aunque',\n",
       " 'poder',\n",
       " 'ay',\n",
       " 'feli',\n",
       " 'sido',\n",
       " 'poco',\n",
       " 'video',\n",
       " 'esos',\n",
       " 'abrazo',\n",
       " 'fui',\n",
       " 'estos',\n",
       " 'pasado',\n",
       " 'super',\n",
       " 'dicen',\n",
       " 'mierda',\n",
       " 'llorar',\n",
       " 'vas',\n",
       " 'ahi',\n",
       " 'sabes',\n",
       " 'felices',\n",
       " 'esas',\n",
       " 'sigo',\n",
       " 'queria',\n",
       " 'sigue',\n",
       " 'mes',\n",
       " 'dio',\n",
       " 'nueva',\n",
       " 'necesito',\n",
       " 'dolor',\n",
       " 'lima',\n",
       " 'cierto',\n",
       " 'queda',\n",
       " 'bonita',\n",
       " 'casi',\n",
       " 'primer',\n",
       " 'han',\n",
       " 'demasiado',\n",
       " 'buenas',\n",
       " 'unos',\n",
       " 'odio',\n",
       " 'comer',\n",
       " 'justo',\n",
       " 'parece',\n",
       " 'estaba',\n",
       " 'viendo',\n",
       " 'falta',\n",
       " 'feo',\n",
       " 'escuchar',\n",
       " 'veces',\n",
       " 'cancion',\n",
       " 'pasar',\n",
       " 'forma',\n",
       " 'contigo',\n",
       " 'hermoso',\n",
       " 'saludo',\n",
       " 'e',\n",
       " 'sol',\n",
       " 'acabo',\n",
       " 'caso',\n",
       " 'lunes',\n",
       " 'tod',\n",
       " 'tal',\n",
       " 'mejo',\n",
       " 'x',\n",
       " 'quede',\n",
       " 'proxima',\n",
       " 'trabajar',\n",
       " 'pronto',\n",
       " 'vi',\n",
       " 'habia',\n",
       " 'pa',\n",
       " 'seguir',\n",
       " 'algun',\n",
       " 'primero',\n",
       " 'xq',\n",
       " 'am',\n",
       " 'hermosa',\n",
       " 'vid',\n",
       " 'peque',\n",
       " 've',\n",
       " 'lado',\n",
       " 'debo',\n",
       " 'importa',\n",
       " 'fuerte',\n",
       " 'momentos',\n",
       " 'sobre',\n",
       " 'dificil',\n",
       " 'exitos',\n",
       " 'jajaj',\n",
       " 'unica',\n",
       " 'llegar',\n",
       " 'foto',\n",
       " 'pude',\n",
       " 'fe',\n",
       " 'chico',\n",
       " 'tomar',\n",
       " 'favor',\n",
       " 'entiendo',\n",
       " 'per',\n",
       " 'quieres',\n",
       " 'haces',\n",
       " 'haya',\n",
       " 'hare',\n",
       " 'nuestro',\n",
       " 'torta',\n",
       " 'dar',\n",
       " 'recien',\n",
       " 'siendo',\n",
       " 'ar',\n",
       " 'tipo',\n",
       " 'regalo',\n",
       " 'sentir',\n",
       " 'chamba',\n",
       " 'muchos',\n",
       " 'sale',\n",
       " 'volver',\n",
       " 'tenia',\n",
       " 'sola',\n",
       " 'horas',\n",
       " 'hacen',\n",
       " 'medio',\n",
       " 'domingo',\n",
       " 'maldito',\n",
       " 'junto',\n",
       " 'so',\n",
       " 'quedo',\n",
       " 'entre',\n",
       " 'toca',\n",
       " 'ptm',\n",
       " 'videos',\n",
       " 'debe',\n",
       " 'cas',\n",
       " 'viene',\n",
       " 'tienen',\n",
       " 'oa',\n",
       " 'nuevos',\n",
       " 'linda',\n",
       " 'fueron',\n",
       " 'musica',\n",
       " 'poner',\n",
       " 'llega',\n",
       " 'cual',\n",
       " 'pobre',\n",
       " 'pueden',\n",
       " 'visto',\n",
       " 'pena',\n",
       " 'saber',\n",
       " 'haciendo',\n",
       " 'mala',\n",
       " 'libre',\n",
       " 'viernes',\n",
       " 'lleno',\n",
       " 'sabado',\n",
       " 'hermana',\n",
       " 're',\n",
       " 'quisiera',\n",
       " 'canciones',\n",
       " 'u',\n",
       " 'misma',\n",
       " 'deseo',\n",
       " 'pe',\n",
       " 'cuanto',\n",
       " 'has',\n",
       " 'quieren',\n",
       " 'unas',\n",
       " 'esperando',\n",
       " 'iba',\n",
       " 'increible',\n",
       " 'd',\n",
       " 'cabeza',\n",
       " 'pais',\n",
       " 'digo',\n",
       " 'encanta',\n",
       " 'tenemos',\n",
       " 'felicidad',\n",
       " 'hago',\n",
       " 'vale',\n",
       " 'ciclo',\n",
       " 'entonces',\n",
       " 'feriado',\n",
       " 'dijo',\n",
       " 'tengas',\n",
       " 'nota',\n",
       " 'muero',\n",
       " 'hayas',\n",
       " 'puedes',\n",
       " 'proximo',\n",
       " 'duele',\n",
       " 'juntos',\n",
       " 'gracia',\n",
       " 'monton',\n",
       " 'ex',\n",
       " 'terminar',\n",
       " 'conmigo',\n",
       " 'twitter',\n",
       " 'hizo',\n",
       " 'mism',\n",
       " 'hambre',\n",
       " 'somos',\n",
       " 'pasan',\n",
       " 'facil',\n",
       " 'horrible',\n",
       " 'cafe',\n",
       " 'sabe',\n",
       " 'deja',\n",
       " 'dormido',\n",
       " 'comida',\n",
       " 'haga',\n",
       " 'concierto',\n",
       " 'viaje',\n",
       " 'jajajajaja',\n",
       " 'creer',\n",
       " 'hagas',\n",
       " 'trafico',\n",
       " 'llego',\n",
       " 'dije',\n",
       " 'noticia',\n",
       " 'tenga',\n",
       " 'aca',\n",
       " 'gratis',\n",
       " 'acaba',\n",
       " 'culpa',\n",
       " 'bie',\n",
       " 'tema',\n",
       " 'clase',\n",
       " 'comprar',\n",
       " 'ven',\n",
       " 'cuerpo',\n",
       " 'adelante',\n",
       " 'csm',\n",
       " 'oy',\n",
       " 'cama',\n",
       " 'mil',\n",
       " 'cumple',\n",
       " 'encuentro',\n",
       " 'hombre',\n",
       " 'fuera',\n",
       " 'llena',\n",
       " 'ense',\n",
       " 'dinero',\n",
       " 'normal',\n",
       " 'pasala',\n",
       " 'celular',\n",
       " 'posible',\n",
       " 'lugar',\n",
       " 'importante',\n",
       " 'recuerdo',\n",
       " 'pensar',\n",
       " 'oye',\n",
       " 'soles',\n",
       " 'poquito',\n",
       " 'examen',\n",
       " 'vino',\n",
       " 'salga',\n",
       " 'chevere',\n",
       " 'ho',\n",
       " 'problemas',\n",
       " 'pone',\n",
       " 'palabras',\n",
       " 'aveces',\n",
       " 'ja',\n",
       " 'san',\n",
       " 'sabia',\n",
       " 'hubiera',\n",
       " 'ok',\n",
       " 'compa',\n",
       " 'venir',\n",
       " 'hombres',\n",
       " 'vivo',\n",
       " 'jamas',\n",
       " 'pense',\n",
       " 'saben',\n",
       " 'vivir',\n",
       " 'jaj',\n",
       " 'ultima',\n",
       " 'quiera',\n",
       " 'enero',\n",
       " 'excelente',\n",
       " 'miercoles',\n",
       " 'diciembre',\n",
       " 'parte',\n",
       " 'cuarto',\n",
       " 'idea',\n",
       " 'miedo',\n",
       " 'hermano',\n",
       " 'vere',\n",
       " 'universidad',\n",
       " 'estamos',\n",
       " 'ropa',\n",
       " 'bello',\n",
       " 'diferente',\n",
       " 'grandes',\n",
       " 'demas',\n",
       " 'tuve',\n",
       " 'dan',\n",
       " 'ando',\n",
       " 'mujer',\n",
       " 'realidad',\n",
       " 'bella',\n",
       " 'nuev',\n",
       " 'quedan',\n",
       " 'luz',\n",
       " 'llevo',\n",
       " 'canal',\n",
       " 'meses',\n",
       " 'par',\n",
       " 'cualquier',\n",
       " 'cosa',\n",
       " 'acuerdo',\n",
       " 'verte',\n",
       " 'chicos',\n",
       " 'rato',\n",
       " 'pase',\n",
       " 'cine',\n",
       " 'calle',\n",
       " 'especial',\n",
       " 'mucha',\n",
       " 'hablar',\n",
       " 'baby',\n",
       " 'jajajaj',\n",
       " 'grande',\n",
       " 'rica',\n",
       " 'empieza',\n",
       " 'manera',\n",
       " 'do',\n",
       " 'todavia',\n",
       " 'trabajos',\n",
       " 'frio',\n",
       " 'algunos',\n",
       " 'maximo',\n",
       " 'lindos',\n",
       " 'manos',\n",
       " 'amigas',\n",
       " 'pasando',\n",
       " 'raro',\n",
       " 'ellos',\n",
       " 'k',\n",
       " 'fecha',\n",
       " 'senti',\n",
       " 'fondo',\n",
       " 'esperar',\n",
       " 'perdon',\n",
       " 'termine',\n",
       " 'podre',\n",
       " 'subir',\n",
       " 'inicio',\n",
       " 'letra',\n",
       " 'equipo',\n",
       " 'cara',\n",
       " 'navide',\n",
       " 'largo',\n",
       " 'encima',\n",
       " 'estudiar',\n",
       " 'like',\n",
       " 'recuerda',\n",
       " 'sean',\n",
       " 'diciendo',\n",
       " 'cansada',\n",
       " 'pos',\n",
       " 'ayuda',\n",
       " 'problema',\n",
       " 'alla',\n",
       " 'queremos',\n",
       " 'naturaleza',\n",
       " 'gust',\n",
       " 'semanas',\n",
       " 'gusto',\n",
       " 'deberia',\n",
       " 'verano',\n",
       " 'compre',\n",
       " 'humor',\n",
       " 'tranquilo',\n",
       " 'ultimas',\n",
       " 'pan',\n",
       " 'decirte',\n",
       " 'tampoco',\n",
       " 'tenido',\n",
       " 'puse',\n",
       " 'salida',\n",
       " 'vacaciones',\n",
       " 'pienso',\n",
       " 'realmente',\n",
       " 'fotos',\n",
       " 'programa',\n",
       " 'chica',\n",
       " 'entro',\n",
       " 'tiempos',\n",
       " 'sensacion',\n",
       " 'alto',\n",
       " 'plaza',\n",
       " 'sepa',\n",
       " 'mente',\n",
       " 'hice',\n",
       " 'olvidada',\n",
       " 'nad',\n",
       " 'maldita',\n",
       " 'amig',\n",
       " 'quier',\n",
       " 'ire',\n",
       " 'doy',\n",
       " 'hacerlo',\n",
       " 'horario',\n",
       " 'igua',\n",
       " 'paz',\n",
       " 'despertar',\n",
       " 'entrar',\n",
       " 'futbol',\n",
       " 'abuela',\n",
       " 'pelicula',\n",
       " 'jueves',\n",
       " 'dejar',\n",
       " 'internet',\n",
       " 'trata',\n",
       " 'uu',\n",
       " 'nosotros',\n",
       " 'hablan',\n",
       " 'or',\n",
       " 'rt',\n",
       " 'llegue',\n",
       " 'sonrisa',\n",
       " 'seman',\n",
       " 'llegado',\n",
       " 'conocer',\n",
       " 'sino',\n",
       " 'amar',\n",
       " 'gustaria',\n",
       " 'helado',\n",
       " 'pueda',\n",
       " 'respuesta',\n",
       " 'encontrar',\n",
       " 'puta',\n",
       " 'yixing',\n",
       " 'cari',\n",
       " 'estare',\n",
       " 'rio',\n",
       " 'on',\n",
       " 'empezar',\n",
       " 'ojos',\n",
       " 'camino',\n",
       " 'youtube',\n",
       " 'distancia',\n",
       " 'juego',\n",
       " 'gana',\n",
       " 'escucha',\n",
       " 'shaky',\n",
       " 'fiesta',\n",
       " 'radio',\n",
       " 'mayoria',\n",
       " 'luna',\n",
       " 'baja',\n",
       " 'simplemente',\n",
       " 'escuchando',\n",
       " 'favorito',\n",
       " 'dices',\n",
       " 'tv',\n",
       " 'llamo',\n",
       " 'luga',\n",
       " 'nomas',\n",
       " 'to',\n",
       " 'irme',\n",
       " 'guapo',\n",
       " 'puesto',\n",
       " 'ustedes',\n",
       " 'the',\n",
       " 'fb',\n",
       " 'hagan',\n",
       " 'minutos',\n",
       " 'pesar',\n",
       " 'siguiente',\n",
       " 'felicitaciones',\n",
       " 'mejore',\n",
       " 'secreto',\n",
       " 'real',\n",
       " 'alegria',\n",
       " 'leer',\n",
       " 'ultimos',\n",
       " 'tomo',\n",
       " 'capitulo',\n",
       " 'hacia',\n",
       " 'gym',\n",
       " 'podria',\n",
       " 'mano',\n",
       " 'mientras',\n",
       " 'haber',\n",
       " 'completa',\n",
       " 'trabaj',\n",
       " 'alegre',\n",
       " 'vaya',\n",
       " 'finale',\n",
       " 'cae',\n",
       " 'estara',\n",
       " 'querer',\n",
       " 'partido',\n",
       " 'oportunidad',\n",
       " 'sentido',\n",
       " 'estado',\n",
       " 'brasil',\n",
       " 'temprano',\n",
       " 'bendiciones',\n",
       " 'pregunto',\n",
       " 'car',\n",
       " 'mrd',\n",
       " 'bajo',\n",
       " 'chocolate',\n",
       " 'dm',\n",
       " 'perugraciasporvaloraramic',\n",
       " 'omar',\n",
       " 'gringaaaa',\n",
       " 'toco',\n",
       " 'dieta',\n",
       " 'dicho',\n",
       " 'porf',\n",
       " 'tristes',\n",
       " 'sigamos',\n",
       " 'felice',\n",
       " 'v',\n",
       " 'anda',\n",
       " 'nombre',\n",
       " 'rojo',\n",
       " 'vemos',\n",
       " 'natural',\n",
       " 'sal',\n",
       " 'profe',\n",
       " 'mentira',\n",
       " 'cuento',\n",
       " 'horribl',\n",
       " 'edad',\n",
       " 'romantica',\n",
       " 'full',\n",
       " 'spotify',\n",
       " 'seres',\n",
       " 'querido',\n",
       " 'pasen',\n",
       " 'alma',\n",
       " 'pongo',\n",
       " 'olvide',\n",
       " 'cel',\n",
       " 'caliente',\n",
       " 'pq',\n",
       " 'madre',\n",
       " 'historia',\n",
       " 'amiguitos',\n",
       " 'hablo',\n",
       " 'peruano',\n",
       " 'mujeres',\n",
       " 'lim',\n",
       " 'segundo',\n",
       " 'blanca',\n",
       " 'mam',\n",
       " 'veras',\n",
       " 'vibras',\n",
       " 'almorzar',\n",
       " 'pica',\n",
       " 'vicicont',\n",
       " 'duro',\n",
       " 'siii',\n",
       " 'aburrido',\n",
       " 'tanta',\n",
       " 'sere',\n",
       " 'enferma',\n",
       " 'entra',\n",
       " 'tio',\n",
       " 'vos',\n",
       " 'jefe',\n",
       " 'empiezo',\n",
       " 'clases',\n",
       " 'cambiar',\n",
       " 'regreso',\n",
       " 'necesita',\n",
       " 'pensaba',\n",
       " 'tres',\n",
       " 'lastima',\n",
       " 'otros',\n",
       " 'vuelvo',\n",
       " 'app',\n",
       " 'ba',\n",
       " 'mar',\n",
       " 'amas',\n",
       " 'tome',\n",
       " 'fea',\n",
       " 'suerte',\n",
       " 'noch',\n",
       " 'sentimiento',\n",
       " 'pocos',\n",
       " 'viajar',\n",
       " 'ingles',\n",
       " 'punto',\n",
       " 'i',\n",
       " 'ves',\n",
       " 'nuestra',\n",
       " 'libro',\n",
       " 'escribir',\n",
       " 'apenas',\n",
       " 'joda',\n",
       " 'conoces',\n",
       " 'alegra',\n",
       " 'seran',\n",
       " 'duda',\n",
       " 'dire',\n",
       " 'serie',\n",
       " 'oh',\n",
       " 'mu',\n",
       " 'ceviche',\n",
       " 'carro',\n",
       " 'mundial',\n",
       " 'escucho',\n",
       " 'ta',\n",
       " 'arequipa',\n",
       " 'fiestas',\n",
       " 'pases',\n",
       " 'viste',\n",
       " 'tesis',\n",
       " 'rapido',\n",
       " 'ala',\n",
       " 'lindas',\n",
       " 'nuestras',\n",
       " 'maneras',\n",
       " 'frase',\n",
       " 'ell',\n",
       " 'peo',\n",
       " 'guste',\n",
       " 'pollo',\n",
       " 'salen',\n",
       " 'tierra',\n",
       " 'dejes',\n",
       " 'digan',\n",
       " 'salud',\n",
       " 'perfecta',\n",
       " 'dulce',\n",
       " 'padre',\n",
       " 'etc',\n",
       " 'are',\n",
       " 'ia',\n",
       " 'tantos',\n",
       " 'tb',\n",
       " 'miguel',\n",
       " 'siente',\n",
       " 'dado',\n",
       " 'ello',\n",
       " 'trabaja',\n",
       " 'princesa',\n",
       " 'vayan',\n",
       " 'sales',\n",
       " 'verla',\n",
       " 'recuerdos',\n",
       " 'china',\n",
       " 'version',\n",
       " 'tard',\n",
       " 'acompa',\n",
       " 't',\n",
       " 'admiro',\n",
       " 'carrera',\n",
       " 'roja',\n",
       " 'perfecto',\n",
       " 'dormida',\n",
       " 'nuestros',\n",
       " 'alg',\n",
       " 'ademas',\n",
       " 'muere',\n",
       " 'min',\n",
       " 'estudio',\n",
       " 'er',\n",
       " 'descanso',\n",
       " 'memes',\n",
       " 'ministro',\n",
       " 'educacion',\n",
       " 'preocupes',\n",
       " 'correo',\n",
       " 'dr',\n",
       " 'jajajajajaja',\n",
       " 'parque',\n",
       " 'habra',\n",
       " 'puntos',\n",
       " 'infinito',\n",
       " 'perro',\n",
       " 'sabemos',\n",
       " 'recordar',\n",
       " 'verdadero',\n",
       " 'pela',\n",
       " 'ale',\n",
       " 'hable',\n",
       " 'rafael',\n",
       " 'disco',\n",
       " 'leche',\n",
       " 'videomtv',\n",
       " 'cerebro',\n",
       " 'imelda',\n",
       " 'terminas',\n",
       " 'trist',\n",
       " 'duela',\n",
       " 'estuviera',\n",
       " 'aoy',\n",
       " 'by',\n",
       " 'teng',\n",
       " 'pendejo',\n",
       " 'jodida',\n",
       " 'amanecer',\n",
       " 'clima',\n",
       " 'gritar',\n",
       " 'peliculas',\n",
       " 'navida',\n",
       " 'red',\n",
       " 'plata',\n",
       " 'faltan',\n",
       " 'empezando',\n",
       " 'sentimientos',\n",
       " 'retos',\n",
       " 'congreso',\n",
       " 'chiste',\n",
       " 'listo',\n",
       " 'comentarios',\n",
       " 'estes',\n",
       " 'vibra',\n",
       " 'contig',\n",
       " 'bebe',\n",
       " 'reales',\n",
       " 'esfuerzo',\n",
       " 'tendre',\n",
       " 'almuerzo',\n",
       " 'bonitas',\n",
       " 'combate',\n",
       " 'nosotro',\n",
       " 'pm',\n",
       " 'futuro',\n",
       " 'media',\n",
       " 'fandom',\n",
       " 'pued',\n",
       " 'muriendo',\n",
       " 'rapid',\n",
       " 'llevar',\n",
       " 'peso',\n",
       " 'siguen',\n",
       " 'trabajando',\n",
       " 'conoci',\n",
       " 'oficina',\n",
       " 'recibir',\n",
       " 'refiero',\n",
       " 'positiva',\n",
       " 'an',\n",
       " 'instagram',\n",
       " 'realida',\n",
       " 'mica',\n",
       " 'mr',\n",
       " 'alcohol',\n",
       " 'estuve',\n",
       " 'apec',\n",
       " 'subi',\n",
       " 'alta',\n",
       " 'estupido',\n",
       " 'recorde',\n",
       " 'campa',\n",
       " 'esperamos',\n",
       " 'gordo',\n",
       " 'series',\n",
       " 'tuvo',\n",
       " 'agua',\n",
       " 'tantas',\n",
       " 'podia',\n",
       " 'area',\n",
       " 'modo',\n",
       " 'pas',\n",
       " 'noches',\n",
       " 'disfrutar',\n",
       " 'jodido',\n",
       " 'gorda',\n",
       " 'general',\n",
       " 'pavo',\n",
       " 'resaca',\n",
       " 'regresar',\n",
       " 'queso',\n",
       " 'pierdo',\n",
       " 'entradas',\n",
       " 's',\n",
       " 'man',\n",
       " 'pierda',\n",
       " 'ley',\n",
       " 'muerte',\n",
       " 'atv',\n",
       " 'gano',\n",
       " 'paneton',\n",
       " 'termino',\n",
       " 'sonido',\n",
       " 'amado',\n",
       " 'mensaje',\n",
       " 'directo',\n",
       " 'alianza',\n",
       " 'intente',\n",
       " 'mando',\n",
       " ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
